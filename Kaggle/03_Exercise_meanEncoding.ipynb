{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_Exercise_meanEncoding.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"o9BmjRue4Mid","colab_type":"text"},"cell_type":"markdown","source":["![](https://cdn-images-1.medium.com/max/1600/1*jX6Gwn1rt4da7e-yUj84IQ.png)\n","\n","### Likelihood Encoding\n","- 也稱為 Impact Encoding 或 Mean Encoding 或 Target Encoding。\n"]},{"metadata":{"id":"cXspzBEl4Mig","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import KFold"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ob6eiSq24Mio","colab_type":"text"},"cell_type":"markdown","source":["### 讀取檔案"]},{"metadata":{"id":"FvQGqwUq4Miq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["PATH = 'data/'\n","train_data = pd.read_table(PATH + 'train.tsv', engine='c')\n","train_data.rename(index=str, columns={'price':'y'},inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sD8fKzZv4Miu","colab_type":"text"},"cell_type":"markdown","source":["### 抓出Category dtypes"]},{"metadata":{"id":"i6SEYe1R4Miw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"8096762b-aa21-4a3b-f23f-97a45a41ae77"},"cell_type":"code","source":["categorical_features = []\n","\n","for dtype, feature in zip(train_data.dtypes, train_data.columns):\n","    if dtype == object:\n","        categorical_features.append(feature)\n","\n","categorical_features"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['name', 'category_name', 'brand_name', 'item_description']"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"h3Go2CrW4Mi4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"799d8c7c-9385-4164-d50b-e2ca24cab3d4"},"cell_type":"code","source":["for f_ in categorical_features:   \n","    print('{} has {} unique items'.format(f_, train_data[f_].nunique()))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["name has 1225273 unique items\n","category_name has 1287 unique items\n","brand_name has 4809 unique items\n","item_description has 1281426 unique items\n"],"name":"stdout"}]},{"metadata":{"id":"W5DnlZIN4Mi9","colab_type":"text"},"cell_type":"markdown","source":["## name, item_description 是文字特徵，所以不能算在內\n","- 注意 **band_name** 缺失值多達42%，本不應該拿去做target encoding\n","- 為了學習方便，移除這些NaN"]},{"metadata":{"id":"SmHLUQNK4Mi9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"0535ad05-d760-4b3f-94b9-c6a1011d55ca"},"cell_type":"code","source":["train_data = train_data[train_data['brand_name'].notnull()]\n","train_data = train_data[train_data['category_name'].notnull()]\n","\n","categorical_features.remove('name')\n","categorical_features.remove('item_description')\n","categorical_features"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['category_name', 'brand_name']"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"wV2CwT_64MjB","colab_type":"text"},"cell_type":"markdown","source":["# Mean encodings without Regularization\n","### 這裡使用兩種範例，只是一個用 `map` 一個是 `transform`\n","\n","   - 範例1\n","\n","\n","```np.corrcoef```\n","\n"]},{"metadata":{"id":"zT6YJEqL4MjC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"2808e13a-c8a7-44f8-f67f-12fb910ec6fe"},"cell_type":"code","source":["for f_ in categorical_features:\n","    # 先算出目標的平均值，也叫「全域均值」\n","    global_mean = train_data['y'].mean()\n","    \n","    # 接者，計算類別的的group mean\n","    item_id_target_mean = train_data.groupby(f_).y.mean()\n","\n","    # 新增一個欄位，賦予類別特徵的group mean 使用 map\n","    train_data['item_target_enc'] = train_data[f_].map(item_id_target_mean)\n","\n","    # 用「全域均值」填充 缺失值，這只是維持好習慣，在本例子沒有NaN，因為是使用「全」train set 去做 encoding\n","    train_data['item_target_enc'].fillna(global_mean, inplace=True) \n","\n","    # 檢視，關聯程度\n","    encoded_feature = train_data['item_target_enc'].values\n","    print('Corr between {} and target is: {}'.format(f_ ,np.corrcoef(train_data['y'].values, encoded_feature)[0][1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Corr between category_name and target is: 0.4465404391861943\n","Corr between brand_name and target is: 0.48512436987737967\n"],"name":"stdout"}]},{"metadata":{"id":"NxCLKZQV4MjG","colab_type":"text"},"cell_type":"markdown","source":["   - 範例2"]},{"metadata":{"id":"vCTu2CBV4MjH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"ba63f6b5-2926-4963-bc65-0bdb1c37f97d"},"cell_type":"code","source":["for f_ in categorical_features:   \n","    \n","    # 先算出目標的平均值，也叫「全域均值」\n","    global_mean = train_data['y'].mean()\n","    \n","    # 接者，計算類別的的group mean\n","    item_id_target_mean = train_data.groupby(f_).y.mean()\n","    \n","    # 新增一個欄位，賦予類別特徵的group mean 使用 transform\n","    train_data['item_target_enc'] = train_data.groupby(f_)['y'].transform('mean')\n","\n","    # 用「全域均值」填充 缺失值，理由同範例一\n","    train_data['item_target_enc'].fillna(global_mean, inplace=True) \n","\n","     # 檢視，關聯程度\n","    encoded_feature = train_data['item_target_enc'].values\n","    print('Corr between {} and target is: {}'.format(f_ ,np.corrcoef(train_data['y'].values, encoded_feature)[0][1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Corr between category_name and target is: 0.4465404391861943\n","Corr between brand_name and target is: 0.48512436987737967\n"],"name":"stdout"}]},{"metadata":{"id":"bWhNbQAT4MjL","colab_type":"text"},"cell_type":"markdown","source":["# 練習時間：\n","## Mean encodings with Regularization\n","###   1. 引入regularization 避免 Overfitting\n","    - 此Regularization並不是L1, L2 Penalty\n","    \n","###   2. 參考指標，檢視跟Target的相關性。 \n","   - 謹記，您的作業的相關性，不應該高過全域的相關性，就是範例1, 2的相關性\n","   - 謹記，低於全域的相關性，不等於一定不會Overfitting\n","   \n","### 3. 請基於 範例1 or 範例2 完成以下\n","1. KFold scheme\n","2. Smoothing\n","3. Smoothing and noising\n","\n","### 4. 練習題採取雙刀流，即簡單的「兩行內」就可以搞定。\n","    - 當然，可以不必理會只限兩行完成，請隨心所欲去發揮！"]},{"metadata":{"id":"1kPefMXy4MjL","colab_type":"text"},"cell_type":"markdown","source":["## 1. KFold scheme\n","#### 本例，在測試是否了解kold scheme，**因為之後很多練習都是基於Kfold**，去做處理，請學員務必務必弄懂。\n","#### 您也可以使用Hold Out scheme。\n","- Hint: \n","- 作法：假設切成N個fold，每次fold loop 取(N-1)份fold 資訊，去套用在剩下的那一份，vice versa.\n","- 只有兩行\n","    - 第一行：指派切割位置\n","    - 第二行：使用條件取代，套用在範例1 or 2 方法\n","\n","您可能會用到，**pandas conditional replace** (google it)\n","\n","\n","```\n","# 這是本題答案數據\n","Corr between category_name and target is: 0.4448439052519606\n","Corr between brand_name and target is: 0.47786979474324226\n","\n","```\n","#### 此題數據如下，請學員盡可能自己寫，去比對上面數據，想不出來再看Answer\n","\n"]},{"metadata":{"id":"BmGHRcoV4MjM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.model_selection import KFold\n","kf = KFold(n_splits = 5, shuffle = False) \n","\n","global_mean = train_data['y'].mean()\n","\n","for f_ in categorical_features:    \n","    \n","    train_data['item_target_enc'] = np.nan # 先賦予nan\n","    \n","    for tr_ind, val_ind in kf.split(train_data): \n","        # tr_ind, val_ind  是切fold後對應到的位置，前是N-1 fold，後者是剩餘fold\n","        \n","        # YOUR CODE GOES HERE:\n","        # 第一行:\n","        \n","        # 第二行:\n","     \n","        \n","    train_data['item_target_enc'].fillna(global_mean, inplace = True)\n","    encoded_feature = train_data['item_target_enc'].values\n","    # You will need to compute correlation like that\n","    corr = np.corrcoef(train_data['y'].values, encoded_feature)[0][1]\n","    print('Corr between {} and target is: {}'.format(f_ ,np.corrcoef(train_data['y'].values, encoded_feature)[0][1]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yghCSOmV4MjQ","colab_type":"text"},"cell_type":"markdown","source":["## 2. Smoothing\n","#### Hint:\n","- 第一行：請參考Slide數學公式的**分子**\n","    - 您可能會用到 `np.multiply` \n","- 第二行：請參考Slide數學公式的**分母**\n","\n","\n","```\n","# 這是本題答案數據\n","Corr between category_name and target is: 0.4439601079786612\n","Corr between brand_name and target is: 0.4569492306386542\n","```\n","#### 此題數據如下，請學員盡可能自己寫，去比對上面數據，想不出來再看Answer\n","\n","\n"]},{"metadata":{"id":"sFa8GsYQ4MjQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# YOUR CODE GOES HERE\n","alpha = 100\n","global_mean = train_data['y'].mean()\n","\n","for f_ in categorical_features:    \n","\n","    train_data['item_target_mean'] = train_data.groupby(f_)['y'].transform('mean')\n","    train_data['target_count'] = train_data.groupby(f_)['y'].transform('count')\n","    \n","    # YOUR CODE GOES HERE\n","    train_data['item_target_enc_smg'] = # 第一行\n","\n","    train_data['item_target_enc_smg'] = # 第二行\n","\n","    encoded_feature = train_data['item_target_enc_smg'].values\n","    corr = np.corrcoef(train_data['y'].values, encoded_feature)[0][1]\n","    print('Corr between {} and target is: {}'.format(f_ ,np.corrcoef(train_data['y'].values, encoded_feature)[0][1]))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t-oeLt8w4MjT","colab_type":"text"},"cell_type":"markdown","source":["## 2-1. Smoothing paper [Daniele Micci-Barreca](https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf)\n","\n","\n","- Hint:\n","1. 練習題的 Equation 4 解釋: \n","    - n 為 個數，\n","    - k 為 min_samples_leaf（設好了，可自行調整）\n","    - f 為 smoothing（設好了，可自行調整）\n","2. 練習題的 Equation 5 解釋: \n","    - B 為 smoothing\n","    - y head 以及 y 為 ？想想\n","    \n","#### 此題答案基於\n","  1. smoothing= 5\n","  2. min_samples_leaf=100\n","  \n","\n","```\n","# 這是本題答案數據\n","Corr between category_name and target is: 0.4423907460273596\n","Corr between brand_name and target is: 0.46616098005914425\n","```\n","#### 此題數據如下，請學員盡可能自己寫，去比對上面數據，想不出來再看Answer， and\n","###  **TA IS NOT ALWAYS RIGHT 如果有錯歡迎指正**\n","\n"]},{"metadata":{"id":"lROTHo5M4MjT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","global_mean = train_data['y'].mean()\n","smoothing= 5 # 調看看其他數值\n","min_samples_leaf=100 # 調看看其他數值\n","\n","for f_ in categorical_features:    \n","\n","    train_data['item_target_mean'] = train_data.groupby(f_)['y'].transform('mean')\n","    train_data['target_count'] = train_data.groupby(f_)['y'].transform('count')\n","    \n","    # Please refer Paper equation 4\n","    # YOUR CODE GOES HERE \n","\n","    \n","    # Please refer Paper equation 5\n","    # YOUR CODE GOES HERE \n","\n","\n","    encoded_feature = train_data['item_target_enc_smg'].values\n","    corr = np.corrcoef(train_data['y'].values, encoded_feature)[0][1]\n","    print('Corr between {} and target is: {}'.format(f_ ,np.corrcoef(train_data['y'].values, encoded_feature)[0][1]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0_0nQdzy4MjX","colab_type":"text"},"cell_type":"markdown","source":["## 3. Smoothing and Noising\n","\n","[Owen Zhan ](https://www.linkedin.com/in/owen-zhang-363aa051)常使用的方法\n","- 現職\tHedge Fund, DataRobot\n","- 曾任\tMeForo (USA), Inc, DataRobot, AIG\n","\n","#### 加入雜訊，並不是一個很好的方法，雜訊多寡沒有準則\n","- 雜訊太大，雜訊會主宰一切\n","- 雜訊太小，那乾脆不要加\n","\n","\n","- Hint:\n","    - 此題做法承接第二題，只是把第二題加入雜訊\n","    - 此題重點在維度 :-)\n","    \n","使用函數\n","`np.random.randn`\n","\n","#### 此題答案基於\n","1. noise_level = 0.05，\n","2. alpha = 100\n","\n","\n","```\n","# 這是本題答案數據\n","Corr between category_name and target is: 0.4423907460273596\n","Corr between brand_name and target is: 0.46616098005914425\n","```\n","#### 此題數據如下，請學員盡可能自己寫，去比對上面數據，想不出來再看Answer\n","\n","\n"]},{"metadata":{"id":"lRz0Eqgi4MjY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","alpha = 100 # 可以調整\n","global_mean = train_data['y'].mean()\n","noise_level = 0.05 # 可以調整這裡 (standard dev)\n","\n","for f_ in categorical_features:    \n","\n","    train_data['item_target_mean'] = train_data.groupby(f_)['y'].transform('mean')\n","    train_data['target_count'] = train_data.groupby(f_)['y'].transform('count')\n","    \n","    \n","    train_data['item_target_enc_smg'] = np.multiply(train_data['item_target_mean'] ,train_data['target_count'] ) + global_mean * alpha\n","    train_data['item_target_enc_smg'] = train_data['item_target_enc_smg'] / (train_data['target_count'] + alpha)\n","    \n","    # YOUR CODE GOES HERE\n","    encoded_feature = # Only 一行而已自己想～～～～\n","    \n","    corr = np.corrcoef(train_data['y'].values, encoded_feature)[0][1]\n","    print('Corr between {} and target is: {}'.format(f_ ,np.corrcoef(train_data['y'].values, encoded_feature)[0][1]))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t5G1nXE54Mjc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}
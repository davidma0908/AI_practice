{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/1600/1*jX6Gwn1rt4da7e-yUj84IQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 請先在terminal執行\n",
    "`pip install unidecode`\n",
    "\n",
    "`pip install psutil`\n",
    "\n",
    "- 如果psutil失敗請把下面這個 cell，comment起來\n",
    "\n",
    "Example: \n",
    "```\n",
    "def cpuStats():\n",
    "    \"\"\" @author: RDizzl3 @address: https://www.kaggle.com/rdizzl3\"\"\"\n",
    "    print(\"########## CPU STATS ############\")\n",
    "    #pid = os.getpid()\n",
    "    #print(pid)\n",
    "    #py = psutil.Process(pid)\n",
    "    #memoryUse = py.memory_info()[0] / 2. ** 30\n",
    "    #print('memory GB:', memoryUse)\n",
    "    #print(\"########## CPU STATS ############\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, psutil\n",
    "\n",
    "\n",
    "def cpuStats():\n",
    "    \"\"\" @author: RDizzl3 @address: https://www.kaggle.com/rdizzl3\"\"\"\n",
    "    print(\"########## CPU STATS ############\")\n",
    "    pid = os.getpid()\n",
    "    print(pid)\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0] / 2. ** 30\n",
    "    print('memory GB:', memoryUse)\n",
    "    print(\"########## CPU STATS ############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "from collections import Counter\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import Ridge, SGDRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, Normalizer, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from multiprocessing import Process, Pool\n",
    "import functools\n",
    "from scipy.special import erfinv\n",
    "from scipy.sparse.linalg import norm\n",
    "import re\n",
    "import unidecode\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = 'data/'\n",
    "train = pd.read_table(PATH + 'train.tsv', engine='c')\n",
    "test = pd.read_table(PATH + 'test.tsv', engine='c')\n",
    "\n",
    "train = train.loc[train.price > 0]\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "y = np.log1p(train[\"price\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 幫助函數，請忽視下面這個Cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hash_binary = True\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='No description yet', inplace=True)\n",
    "\n",
    "    \n",
    "def preprocess(text): # 這是之前的作業的簡易版\n",
    "    non_alphanums = re.compile(u'[^A-Za-z0-9]+')\n",
    "    # regex for short = re 請參考 http://ccckmit.wikidot.com/regularexpression \n",
    "    \n",
    "    text = unidecode.unidecode(text)\n",
    "    text = str(text).lower()\n",
    "    return u\" \".join(\n",
    "        [x for x in [y for y in non_alphanums.sub(' ', text).strip().split(\" \")]])\n",
    "        # strip split 請參考 http://ericbbs.blogspot.tw/2009/07/python-strip-split.html\n",
    "        # [x for x in.....] 這文言文：是 list comprehension\n",
    "\n",
    "    \n",
    "### 以下是multithread ，請自行忽略  \n",
    "\n",
    "def multi_hash(data=None, hash_vec=None, n_jobs=4):\n",
    "    \"\"\"\n",
    "      Only available for hashing vectorizers since they are stateless\n",
    "      This would not work for TFIDF or Count transformations\n",
    "      \"\"\"\n",
    "    p = Pool(n_jobs)\n",
    "    csr_parts = p.map(hash_vec.fit_transform, np.array_split(data, n_jobs))\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return vstack(csr_parts).tocsr\n",
    "\n",
    "def multi_apply(df=None, feat_list=None, func=None, axis=0, raw=True, n_jobs=4):\n",
    "    \"\"\"\n",
    "      @author: Olivier @address: https://www.kaggle.com/ogrellier\n",
    "      Function that creates process pools to perform actions on DataFrames\n",
    "      \"\"\"\n",
    "    p = Pool(n_jobs)\n",
    "    f_ = p.map(functools.partial(apply_func, func=func, axis=axis, raw=raw),\n",
    "               np.array_split(df[feat_list], n_jobs))\n",
    "    f_ = pd.concat(f_, axis=0, ignore_index=True)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return f_.values\n",
    "\n",
    "def apply_func_series(data=None, func=None):\n",
    "    \"\"\"\n",
    "      @author: Olivier @address: https://www.kaggle.com/ogrellier\n",
    "      Utility function that allows multi processing on series\n",
    "      \"\"\"\n",
    "    return data.apply(func)\n",
    "\n",
    "def multi_apply_series(df=None, feature=None, func=None, n_jobs=4):\n",
    "\n",
    "    p = Pool(n_jobs)\n",
    "    f_ = p.map(functools.partial(apply_func_series, func=func),\n",
    "               np.array_split(df[feature], n_jobs))\n",
    "    f_ = pd.concat(f_, axis=0, ignore_index=True)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return f_.values\n",
    "    \n",
    "\n",
    "def apply_func(data=None, func=None, axis=0, raw=True):\n",
    "\n",
    "    return data.apply(func, axis=axis, raw=raw)\n",
    "\n",
    "\n",
    "def preprocess_text_features(df):\n",
    "    \"\"\"\n",
    "      Utility function to apply text pre-processing by Kueipo to name, brand and description\n",
    "      but in parallel\n",
    "      \"\"\"\n",
    "    df[\"item_description\"] = multi_apply_series(df=df[[\"item_description\"]],\n",
    "                                                feature=\"item_description\",\n",
    "                                                func=preprocess,\n",
    "                                                n_jobs=4)\n",
    "    df[\"name\"] = multi_apply_series(df=df[[\"name\"]],\n",
    "                                    feature=\"name\",\n",
    "                                    func=preprocess,\n",
    "                                    n_jobs=4)\n",
    "    \n",
    "def get_hashing_features(df, Hash_binary, start_time):\n",
    "    # df = pd.concat([train, test])\n",
    "    dim = 20\n",
    "    ha = HashingVectorizer(\n",
    "        n_features=2 ** dim,\n",
    "        ngram_range=(1, 2),\n",
    "        norm=None,\n",
    "        alternate_sign=False,\n",
    "        binary=Hash_binary\n",
    "        # stop_words='english'\n",
    "    )\n",
    "\n",
    "    X_name = ha.fit_transform(df['name'])\n",
    "    cpuStats()\n",
    "    X_name += ha.fit_transform(df['item_description'])\n",
    "    cpuStats()\n",
    "    \n",
    "    print('[{}] Finished hashing'.format(time.time() - start_time))\n",
    "    return X_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106.72351503372192] Finished hashing\n",
      "Shape of this Sparse Matrix: 1048576\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "handle_missing_inplace(train) # 處理 NaN \n",
    "preprocess_text_features(df=train) # 文字預處理，前一份exercise\n",
    "\n",
    "csr_trn = get_hashing_features(train, Hash_binary, start_time) # Hash Trick\n",
    "\n",
    "print('Shape of this Sparse Matrix: {}'.format(csr_trn.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徵值有一百零四萬多，請練習使用Lasso選特徵\n",
    "- 請參看[SGDRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor) attributes\n",
    "`coef_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106.75732517242432] Starting SGD l1 selection\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=1e-06, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', max_iter=30, n_iter=None, penalty='l1',\n",
       "       power_t=0.25, random_state=1, shuffle=True, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply L1 feature selection\n",
    "print('[{}] Starting SGD l1 selection'.format(time.time() - start_time))\n",
    "sgd_l1 = SGDRegressor(max_iter=30, penalty=\"l1\", random_state=1, alpha=1e-6)\n",
    "sgd_l1.fit(csr_trn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習區：\n",
    "- Hint 1:\n",
    "    - Lasso 會陽痿一堆欄位係數\n",
    "- Hint 2:\n",
    "    - 您要做的事情就是挑選係數範圍，自訂\n",
    "    \n",
    "請看被陽痿後的係數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05418402, 0.00060792, 0.        , ..., 0.00335522, 0.        ,\n",
       "       0.02573101])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_l1.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 1:\n",
    "      - 請檢視最大值，最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_l1.coef_.min(), sgd_l1.coef_.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 2:\n",
    "- 請挑選係數範圍\n",
    "- Hint:\n",
    "    1. 請記得用絕對值`np.abs()`，如果有做 Excercise 1，會知道為什麼\n",
    "    2. 請轉成 `np.array()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[180.59474921226501] Finished SGD l1 selection\n",
      "Features reduced from    1048576 to     446994\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Finished SGD l1 selection'.format(time.time() - start_time))\n",
    "\n",
    "good_feats = np.abs(np.array(sgd_l1.coef_)) > 1e-6 # lower bound 自行斟酌，可以參考參數設定 alpha=1e-6\n",
    "\n",
    "print(\"Features reduced from %10d to %10d\" % (csr_trn.shape[1], int(good_feats.sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 怎麼套用到train/test ?\n",
    "    - `csr_trn = csr_trn[:, good_feats]`\n",
    "    - `csr_test = csr_test[:, good_feats]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徵數從一百萬個降到，四十四萬，過程就跟Slide一樣，這就是Lasso Selection，其中一種做法。\n",
    "- Quite ez right ?\n",
    "#### Important Note:\n",
    "    - 請注意Overfitting，L1 Selection不一定會達到預期的目標，請研究 Lasso適合用在什麼數據，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

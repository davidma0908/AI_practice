{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_DeepLearning_without_framework_BP_method.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"bd9AKvKhtZuE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qyS-8MEntZuJ","colab_type":"text"},"cell_type":"markdown","source":["# Create network by layers and use gradient descent with computation graph (Backprop, BP)\n","### 可以把每個操作拆解成局部區域進行微分: Simple examples\n","\n","超商裡的蘋果 1 顆 100 元, 橘子 1 顆 150 元, 結帳時要抽 10% 營業稅, 如果小明買了 2 顆蘋果和 3 顆橘子, 請問若 ... <br>\n","1. 多買一顆蘋果，對結帳價錢的變化?\n","2. 蘋果一顆的價錢漲 1 元時，對結帳價錢的變化?\n","3. 營業稅多 100% 時，對價錢的變化?"]},{"metadata":{"id":"UFu0WJ2_tZuJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#--------------------forward and backward function-----------------------\n","class mul_layer():\n","    def __init__(self):\n","        self.x = None\n","        self.y = None\n","    def forward(self, x, y):\n","        self.x = x\n","        self.y = y\n","        out = x * y\n","        return out\n","    def backward(self, dout):\n","        dx = dout * self.y\n","        dy = dout * self.x\n","        return dx, dy"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G_R636pbtZuM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"3530d6de-08fa-454a-a2a5-b0a9f0a4eb8a"},"cell_type":"code","source":["# ----------- #\n","n_apples = 2\n","price_per_apple = 100\n","tax = 1.1\n","\n","\n","# Build network\n","mul_apple_layer = mul_layer()\n","mul_or_layer=mul_layer()\n","mul_tax_layer = mul_layer()\n","\n","# forward\n","apple_price = mul_apple_layer.forward(price_per_apple, n_apples)\n","price = mul_tax_layer.forward(apple_price, tax)\n","\n","# backward\n","dprice = 1 # because we assume using linear function, which y = x, dy/dx = 1\n","d_total_apples_price, d_tax = mul_tax_layer.backward(dprice)\n","d_price_per_apple, d_apple_num = mul_apple_layer.backward(d_total_apples_price)\n","\n","# results\n","print(\"final price: %i\" % price)\n","print(\"1. d_price_per_apple: %.2f\" % d_price_per_apple)\n","print(\"2. d_Apple_num: %.2f\" % d_apple_num)\n","print(\"3. d_Tax: %.2f\" % d_tax)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["final price: 220\n","1. d_price_per_apple: 2.20\n","2. d_Apple_num: 110.00\n","3. d_Tax: 200.00\n"],"name":"stdout"}]},{"metadata":{"id":"TP4Rb7-XtZuQ","colab_type":"text"},"cell_type":"markdown","source":["## Exercise\n","- 多買一顆橘子，對結帳價錢的變化?\n","- 橘子一顆的價錢漲 1 元時，對結帳價錢的變化?"]},{"metadata":{"id":"ADbcPW0btZuR","colab_type":"text"},"cell_type":"markdown","source":["# Create network by layers and use BP\n","開始前再次強調，BP只是對於deep learning在計算偏微分的技巧，是加速計算optimizer的工具，有了BP我們可以批次對不同的參數計算偏微分，\n","計算兩次neural network(forward pass 和 backward pass)和一些小計算就能完成所有的\n","取代掉前一個章節取偏微分的算法:一次整個neural network計算(嚴格說起來是兩次)只算出一個參數的偏微分。\n","\n","\n","(分別執行完範例後應該就能感受到兩者速度上的差異)\n","\n","\n","https://www.youtube.com/watch?v=ibJpTrp5mcE\n"]},{"metadata":{"id":"jm4020h7tZuR","colab_type":"text"},"cell_type":"markdown","source":["#### 公式重提　\n","#### 就老師的範例而言\n","<img src=\"aaa.jpg\">\n","#### 則:\n","## $\\frac{\\partial C(\\theta)}{\\partial w_1}=\\frac{\\partial z}{\\partial w_1}\\frac{\\partial C(\\theta)}{\\partial z}=x_1 \\times \\sigma'(z) (w_3\\times\\frac{\\partial C(\\theta)}{\\partial z'}+w_4\\times\\frac{\\partial C(\\theta)}{\\partial z''} )=$\n","## $x_1 \\times \\sigma'(z) (w_3\\times\\frac{\\partial y_1}{\\partial z'}\\frac{\\partial C(\\theta)}{\\partial y_1}+w_4\\times\\frac{\\partial y_2}{\\partial z''}\\frac{\\partial C(\\theta)}{\\partial y_2} )$\n","## $=forword pass後得到的值\\times backword pass後得到的值$"]},{"metadata":{"id":"hNeWXmUytZuS","colab_type":"text"},"cell_type":"markdown","source":["## Real example\n","#### Define other layer functions\n","與第一份範例不同，我們除了要列出layer中的計算過程(forward),還要為了BP寫backward的function\n","\n","btw backward其實就像是反過來的network, activation function變成原來activation function的微分"]},{"metadata":{"id":"hnUAMx1NtZuT","colab_type":"text"},"cell_type":"markdown","source":["### part one: sigmoid \n","### $sigmoid(z)=a=\\frac{1}{1+e^{-x}}$\n","### $sigmoid'(z)=a(1-a)$"]},{"metadata":{"id":"AQ4uCDH9tZuU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Define activation(sigmoid)_layer functions\n","class sigmoid_layer():\n","    def __init__(self):\n","        self.out = None #這裡的out就是影片與上面方程式中的a\n","    def forward(self, x):\n","        out = 1 / (1 + np.exp(-x))\n","        self.out = out  #記錄在class中作計算backward備用\n","        \n","        return out\n","    def backward(self, dout):\n","        dx = dout * (1.0 - self.out) * self.out # dL/da * da/dz = dL/da * y * (1-y)\n","        \n","        return dx\n","# Define affine_layer functions    \n","class affine_layer():\n","    def __init__(self, W, b):\n","        self.W = W\n","        self.b = b\n","        self.x = None\n","        self.original_x_shape = None\n","        self.dW =None\n","        self.db = None\n","        \n","    def forward(self, x):\n","        self.original_x_shape = x.shape\n","        x = x.reshape(x.shape[0], -1)\n","        \n","        self.x = x #記錄在class中作計算backward備用\n","        out = np.dot(self.x, self.W) + self.b\n","        \n","        return out\n","    \n","    def backward(self, dout):\n","        dx = np.dot(dout, self.W.T)\n","        \n","        self.dW = np.dot(self.x.T, dout)\n","        self.db = np.sum(dout, axis = 0)\n","        dx = dx.reshape(*self.original_x_shape)\n","        \n","        return dx\n","    \n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x.T # Transpose it\n","        x = x - np.max(x, axis=0)\n","        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","        return y.T \n","    return np.exp(x) / np.sum(np.exp(x))\n","\n","# define cross_entropy\n","def cross_entropy(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","        \n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","             \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n","\n","class softmax_with_crossentropy():\n","    def __init__(self):\n","        self.loss = None\n","        self.y = None # softmax output\n","        self.t = None # target (ground-truth)\n","        \n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)#將predict後的結果餵入最後的softmax作為最後的預測(y_pred)\n","        self.loss = cross_entropy(self.y, self.t)\n","        \n","        return self.loss\n","    def backward(self, dout = 1):\n","        batch_size = self.t.shape[0]\n","        dx = (self.y - self.t) / batch_size #以predict 與 truth的差值除以batch size, \n","                                            #作為bp的起點(事實上就是Loss func對最後一步z的偏微分)\n","        \n","        return dx"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z0grKcRctZuW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from collections import OrderedDict # this is a built-in function -- dictionary with order\n","\n","#ordereddic和一般dict差別的參考範例：https://blog.csdn.net/liangguohuan/article/details/7088304\n","\n","class Two_layerNet_bp:\n","    def __init__(self, \n","                 input_size, \n","                 hidden_size, \n","                 output_size, \n","                 weight_init_std = 0.01):\n","        #------------------------------定義好要用的陣列---------------------------------\n","        self.params = {}#用dict存放所有陣列\n","        self.params['w1'] = weight_init_std * np.random.randn(input_size, hidden_size) #以亂數作為w的起始值\n","        self.params['b1'] = np.zeros(hidden_size) #以0作為b的起始值\n","        self.params['w2'] = weight_init_std * np.random.randn(hidden_size, output_size) #以亂數作為w的起始值\n","        self.params['b2'] = np.zeros(output_size) #以0作為b的起始值\n","        \n","        #------------------------------build network---------------------------------\n","        self.layers = OrderedDict()#用ordereddic存放所有layers\n","        self.layers['affine_1'] = affine_layer(self.params['w1'], self.params['b1'])\n","        self.layers['sigmoid_1'] = sigmoid_layer()\n","        self.layers['affine_2'] = affine_layer(self.params['w2'], self.params['b2'])\n","        # output layer\n","        self.lastlayer = softmax_with_crossentropy()\n","    #---------------------predict----------------------    \n","    def predict(self, x):\n","        # forward\n","        for layer in self.layers.values(): #從第一層開始,將最初的x餵進layer,並將輸出當作下一層layer的input持續做到倒數第二層(不包含最後的softmax層)，得到output\n","            x = layer.forward(x) \n","        \n","        return x\n","#--------------------------lost function(with cross entropy)---------------              \n","    def loss(self, x, y_true):\n","        y_pred = self.predict(x) #調用上面的predict function\n","        return self.lastlayer.forward(y_pred, y_true)#仔細看到上個cell最後一個class \"softmax_with_crossentropy()\"的\"forward\"，\n","                                                    #除了計算softmax、cross_entropy也將計算結果、y_pred、y_true記錄在softmax_with_crossentropy()內,\n","                                                    #在調用backward時就不用再餵入y_pred,y_true\n","#----------------------------------accuracy--------------------------------       \n","    def compute_acc(self, x, y_true):\n","        y_pred = self.predict(x)\n","        # take argmax\n","        y_pred = y_pred.argmax(axis = 1) \n","        y_true = y_true.argmax(axis = 1)\n","        \n","        acc = np.sum(y_pred == y_true) / len(y_true)\n","        return acc\n","#-----------------------------optimizer(gradient)---------------------------\n","    def gradient(self, x, t):\n","        # forward path\n","        self.loss(x, t) #這步回頭看function就會了解，除了做了loss，也做loss前的predict，另外predict同時各層affine layer 的input \"x\"、\n","                        #optimizer(cross)的input \"t\" and \"y\"、activation function layer的output \"out\"與也會記錄在分別的class中,\n","                        #而各層affine layer的x就是'partial(z)/partial(w)'，所以別以為怎麼少了許多步驟。\n","        \n","        # backward\n","        dout = 1\n","        dout = self.lastlayer.backward(dout)\n","        \n","        layers = list(self.layers.values())\n","        layers.reverse() #將layers反過來準備開始backward\n","        for layer in layers:#backward 開始!!!!\n","            dout = layer.backward(dout)#分別計算dw、db並各自記錄在自己的layer中\n","        \n","        # gradient init and setting\n","        grads = {}\n","        grads['w1'] = self.layers['affine_1'].dW\n","        grads['b1'] = self.layers['affine_1'].db\n","        grads['w2'] = self.layers['affine_2'].dW\n","        grads['b2'] = self.layers['affine_2'].db\n","        \n","        return grads"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r3ISfvHRtZuY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"7c55c3d4-c4cc-48d8-a29c-60a0a7b330c0"},"cell_type":"code","source":["from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import time\n","\n","'''-----------------------------------資料前處理---------------------------------------------'''\n","digits = load_digits()\n","x_, y_ = digits.data, digits.target\n","\n","\n","#---------將資料做One-Hot Encoding---------\n","y_one_hot = np.zeros((len(y_), 10))\n","y_one_hot[np.arange(len(y_)), y_] = 1 \n","#---------one way of normalization--------\n","x_ = x_ / x_.max() # normailze it to 0 - 1 標準化\n","\n","#----------------------------資料training set, testing set 分割---------------------------------------\n","x_train, x_test, y_train, y_test = train_test_split(x_, y_one_hot, test_size = 0.1, stratify = y_)\n","'''--------------------------------建立List準備用來存過程中的acc與loss變化-------------------------'''\n","# define training settings ------------\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","'''-----------------------------------參數設定與創建新模型--------------------------------------------'''\n","iters_num = 5001\n","train_size = x_train.shape[0] # numbers of training samples\n","bz = 100 # batch size\n","lr = 0.1 # learning rate\n","\n","network = Two_layerNet_bp(input_size=64, hidden_size=25, output_size=10)\n","'''-----------------------------------start_training---------------------------------------------'''\n","starttime=time.time()\n","for i in tqdm(range(iters_num)):\n","    batch_mask = np.random.choice(train_size, bz)\n","    x_batch = x_train[batch_mask]\n","    y_batch = y_train[batch_mask]\n","    \n","    grad = network.gradient(x_batch, y_batch) # already contain a feed-forward processing in this step\n","    \n","    for key in (\"w1\", \"b1\", \"w2\", \"b2\"):\n","        network.params[key] -= lr * grad[key] #更新參數\n","        \n","    this_loss = network.loss(x_batch, y_batch)\n","    train_loss_list.append(this_loss)\n","    \n","     #每執行100次紀錄一次 \n","    if i % 100 == 0:\n","        # compute accuracy for every 100 updates\n","        train_acc = network.compute_acc(x_train, y_train)#計算每個training set的 acc\n","        test_acc = network.compute_acc(x_test, y_test)#記算每個testing set的 acc\n","        \n","        train_acc_list.append(train_acc)#記錄每個training set的 acc\n","        test_acc_list.append(test_acc)#記錄每個testing set的 acc\n","timerange=time.time()-starttime        \n","print('總共費時：',timerange)  \n","   "],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 5001/5001 [00:02<00:00, 1839.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["總共費時： 2.7217187881469727\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"metadata":{"id":"44KqrQ3mtZuj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"6a80a099-9eca-42a7-f865-29c4e85568e1"},"cell_type":"code","source":["  '''-----------------------------------圖形化---------------------------------------------'''  \n","#------------------------------------------------------plot------------------------------------------------------\n","#print(\"Train accuarcy, Test accuracy | \" + str(train_acc) + \", \" + str(test_acc))\n","    \n","#lose圖\n","\n","plt.plot(np.arange(len(train_loss_list)), train_loss_list, 'b-')\n","plt.show()\n","\n","#training/validation accuracy圖\n","plt.plot(np.arange(len(train_acc_list)), train_acc_list, 'b-', label = 'training accuracy')\n","plt.plot(np.arange(len(test_acc_list)), test_acc_list, 'r-', label = 'validation accuracy')\n","plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["loss趨勢圖，x座標是看過單筆資料的次數，y軸是loss值\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'train_loss_list' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-1b6295e0c725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#lose圖\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss趨勢圖，x座標是看過單筆資料的次數，y軸是loss值'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acc趨勢圖，x座標是看過單筆資料的次數，y軸是正確率'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_loss_list' is not defined"]}]},{"metadata":{"id":"Tt9d0bUJtZun","colab_type":"text"},"cell_type":"markdown","source":["## 觀看資料相同次數，更新參數相同次數，可以發現BP速度加快非常多!!"]},{"metadata":{"id":"SqnmfkPbtZuo","colab_type":"text"},"cell_type":"markdown","source":["## Exercise\n","分別試著完成以下動作\n","1. 試著改變hidden size並比較執行結果 (easy 找到關鍵參數修改值即可)\n","2. 試著再加一層hidden layer並比較執行結果(little hard 得在所有跟layer有關的多個地方做增加) "]},{"metadata":{"id":"VMb2RCRatZuo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}
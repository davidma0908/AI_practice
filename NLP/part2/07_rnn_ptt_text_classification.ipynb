{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07_rnn_ptt_text_classification.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"6HbiNb7tuPNW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","import pickle\n","from gensim.models import word2vec\n","import random\n","\n","from sklearn.preprocessing import StandardScaler\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IlAf6k8euPNa","colab_type":"text"},"cell_type":"markdown","source":["### Step 0. Loading dataset"]},{"metadata":{"id":"lvEhxA7_uPNb","colab_type":"text"},"cell_type":"markdown","source":["#### Step 0.1 load article cutted and article df and define y"]},{"metadata":{"id":"EZwp-WsZuPNb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["with open(\"../article_cutted\", \"rb\") as file:\n","    docs = pickle.load(file)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bFQvhCnouPNd","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["df = pd.read_csv('../data/article_preprocessed.csv')\n","diff_threshold = 20\n","df = df[abs(df['push']-df['boo']) > diff_threshold].copy()\n","df['type'] = np.clip(df['push']-df['boo'], 0, 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"B8XHk6SguPNg","colab_type":"text"},"cell_type":"markdown","source":["#### Step 0.2 create word id mapping and word vector"]},{"metadata":{"id":"N0evuFZJuPNg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["w2v = word2vec.Word2Vec.load('../word2vec_model/CBOW')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TTqL7HAPuPNi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["word2id = {k:i for i, k in enumerate(w2v.wv.vocab.keys())}\n","id2word = {i:k for k, i in word2id.items()}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"crdG_vUmuPNk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["words_len = len(word2id)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NBJ8KKsiuPNn","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["embedding = np.zeros((words_len+1, 256))\n","for k, v in word2id.items():\n","    embedding[v] = w2v.wv[k]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ww_Yjk2auPNp","colab_type":"text"},"cell_type":"markdown","source":["#### Step 0.3 sentence to seq transform"]},{"metadata":{"id":"bKE_MSpIuPNq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["input_length = 80\n","docs_id = []\n","for doc in docs:\n","    text = doc[:input_length]\n","    ids = [words_len+1]*input_length\n","    ids[:len(text)] = [word2id[w] if w in word2id else words_len+1 for w in text]\n","\n","    docs_id.append(ids)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z9vZqbvGuPNr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"outputId":"9995fffd-ce85-4c8e-822f-771fbf8b40fd"},"cell_type":"code","source":["print(docs[0])\n","print(docs_id[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['韓瑜', '協志', '前妻', '正', '女演員', '周子', '瑜', 'TWICE', '團裡裡面', '台灣', '人', '正', '兩個', '要當', '鄉民', '老婆', '選', '五樓', '真', '勇氣']\n","[0, 1, 2, 3, 4, 5, 6, 7, 100035, 8, 9, 3, 10, 11, 12, 13, 14, 15, 16, 17, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035, 100035]\n"],"name":"stdout"}]},{"metadata":{"id":"QIgXcRXDuPNv","colab_type":"text"},"cell_type":"markdown","source":["### Step 1. Data preprocessing"]},{"metadata":{"id":"nz_CRjP1uPNw","colab_type":"text"},"cell_type":"markdown","source":["#### Step 1.1 Creating Training and Testing sets and creating generator"]},{"metadata":{"id":"eVjfzGNDuPNx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7n4Lfma2uPN0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["train, test = train_test_split(df, test_size=0.2, shuffle=True, stratify=df['type'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3QCQnnIhuPN2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def train_data_generator(df, bz, docs_id):\n","    # bz: batch size \n","    \n","    dfs = [sub_df for key,sub_df in df.groupby('type')]\n","    df_n = len(dfs)\n","    \n","    docs_id = np.array(docs_id)\n","    while True:\n","        selected = pd.concat([sub_df.sample(int(bz/2)) for sub_df in dfs], axis=0)\n","        selected = selected.sample(frac=1)\n","        x = docs_id[selected['idx']]\n","        y = selected.as_matrix(columns=['type'])\n","                    \n","        yield x, y\n","        \n","def test_data_generator(df, docs_id):\n","    docs_id = np.array(docs_id)\n","    x = docs_id[df['idx']]\n","    y = df.as_matrix(columns=['type'])\n","\n","    return x, y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iOwTLmDkuPN4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["X_test, Y_test = test_data_generator(test, docs_id) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"C1j599yZuPN6","colab_type":"text"},"cell_type":"markdown","source":["### Let's create the RNN"]},{"metadata":{"id":"Eib4FB-CuPN7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["epochs = 100\n","batch_size = 32\n","update_per_epochs = 100\n","\n","learning_rate=0.001\n","hidden_layer_size=64\n","number_of_layers=1\n","dropout=True\n","dropout_rate=0.8\n","number_of_classes=1\n","gradient_clip_margin=4\n","wv=embedding"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GHIAFkXduPN9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def LSTM_cell(hidden_layer_size, batch_size, number_of_layers, dropout=True, dropout_rate=0.8):\n","    def get_LSTM(hidden_layer_size, dropout, dropout_rate):\n","        layer = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size)\n","\n","        if dropout:\n","            layer = tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n","            \n","        return layer\n","    \n","    cell = tf.contrib.rnn.MultiRNNCell([get_LSTM(hidden_layer_size, dropout, dropout_rate) for _ in range(number_of_layers)])\n","\n","    init_state = cell.zero_state(batch_size, tf.float32)\n","\n","    return cell, init_state"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wfl64n1GuPOA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def output_layer(lstm_output, out_size):\n","    x = lstm_output[:, -1, :]\n","    output = tf.layers.dense(inputs= x, units= out_size, activation = tf.nn.sigmoid)\n","    return output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c497NKMBuPOC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def opt_loss(logits, targets, learning_rate, grad_clip_margin):\n","    \n","    loss = tf.reduce_sum(tf.pow(logits - targets, 2))/batch_size\n","    \n","    #Cliping the gradient loss\n","    optimizer = tf.train.AdamOptimizer(learning_rate)\n","    gradients = optimizer.compute_gradients(loss)\n","\n","    capped_gradients = [(tf.clip_by_value(grad, (-1)*grad_clip_margin, grad_clip_margin), var) for grad, var in gradients if grad is not None]\n","    \n","    train_optimizer = optimizer.apply_gradients(capped_gradients)\n","\n","    \n","    return loss, train_optimizer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VRWsII5cuPOD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["main_graph = tf.Graph()\n","sess = tf.Session(graph=main_graph)\n","\n","with main_graph.as_default():\n","    \n","    ##defining placeholders##\n","    with tf.name_scope('input'):\n","        inputs = tf.placeholder(tf.int32, [None, input_length], name='input_data')\n","        targets = tf.placeholder(tf.float32, [None, 1], name='targets')\n","        bz = tf.placeholder(tf.int32, [], name='batch_size')\n","        \n","    ## embedding lookup table\n","    with tf.variable_scope('embedding'):    \n","        em_W = tf.Variable(wv.astype(np.float32), trainable=True)  #wv.shape = (100035, 256)\n","        x = tf.nn.embedding_lookup(em_W, inputs)    #x.shape = (?, 80, 256)\n","        \n","    ##LSTM layer##\n","    with tf.variable_scope(\"LSTM_layer\"):\n","        cell, init_state = LSTM_cell(hidden_layer_size, tf.shape(inputs)[0], number_of_layers, dropout, dropout_rate) \n","        outputs, states = tf.nn.dynamic_rnn(cell, x, initial_state=init_state)\n","    \n","    ##Output layer##   \n","    with tf.variable_scope('output_layer'):\n","        logits = output_layer(outputs, number_of_classes)\n","    \n","    ##loss and optimization##\n","    with tf.name_scope('loss_and_opt'):\n","        loss, opt = opt_loss(logits, targets, learning_rate, gradient_clip_margin)\n","    \n","    init = tf.global_variables_initializer()\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"k-vsxfy3uPOF","colab_type":"text"},"cell_type":"markdown","source":["### Time to train the network"]},{"metadata":{"id":"DoQJCDVYuPOF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["sess.run(init)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fPnPG66kuPOH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","train_generate = train_data_generator(train, batch_size, docs_id)\n","\n","train_loss = []\n","train_auc = []\n","test_loss = []\n","test_auc = []\n","for i in range(epochs):\n","    traind_scores = []\n","    epoch_loss = []\n","    for j in range(update_per_epochs):\n","        X_batch, y_batch = next(train_generate) \n","        \n","        o, c, _ = sess.run([logits, loss, opt], feed_dict={\n","            inputs:X_batch, \n","            targets:y_batch,\n","            bz:np.array(batch_size)\n","        })\n","        \n","        epoch_loss.append(c)\n","        traind_scores.append(roc_auc_score(y_batch, o))\n","    \n","    to, tc = sess.run([logits, loss], feed_dict={\n","        inputs:X_test, \n","        targets:Y_test,\n","        bz:np.array(len(X_test))\n","    })\n","    \n","    train_loss.append(np.mean(epoch_loss))\n","    train_auc.append(np.mean(traind_scores))\n","    test_loss.append(tc)\n","    test_auc.append(roc_auc_score(Y_test, to))\n","    \n","    if (i % 5) == 0:\n","        print('Epoch {}/{}'.format(i, epochs), ' Train loss: {}'.format(np.mean(epoch_loss)), \n","              ' Train auc: {}'.format(np.mean(traind_scores)), \n","             ' Test loss: {}'.format(tc), ' Test auc: {}'.format(roc_auc_score(Y_test, to)))\n"],"execution_count":0,"outputs":[]}]}